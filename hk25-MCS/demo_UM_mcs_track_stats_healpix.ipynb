{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87b660a",
   "metadata": {},
   "source": [
    "# Demonstration of accessing PyFLEXTRKR MCS tracking outputs\n",
    "\n",
    "This notebook is based on Zhe Feng's [Notebook](https://github.com/FlexTRKR/PyFLEXTRKR/blob/main/Notebooks/demo_mcs_track_stats_healpix.ipynb) and modified for UM model and tracking data. It demonstrates how to access and perform simple analysis on the MCS tracking outputs from PyFLEXTRKR. It also gives example code of the basic methods of loading the data using `xarray`, for both the tracks data and the HEALPix pixel-level data, as well as linking the two together. Some simple analysis is done.\n",
    "\n",
    "This example largely follows that developed by Mark Muetzelfeldt, with updates to the pixel-level data in HEALPix format catalog.\n",
    "\n",
    "* Authors:\n",
    "- Torsten Auerswald (t.auerswald@reading.ac.uk)\n",
    "- Zhe Feng (zhe.feng@pnnl.gov)\n",
    "- Mark Muetzelfeldt (mark.muetzelfeldt@reading.ac.uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5719bd",
   "metadata": {},
   "source": [
    "### Track statistics files\n",
    "\n",
    "The MCS tracks statistics dataset are NetCDF4 files. These are laid out as 1D, 2D and 3D arrays of data, with the main coordinate being `tracks`, and variable length data stored in a fixed length array (using a maximum duration of 650, equivalent to an MCS that last for 650 hr). A consequence of this is that all the data over the duration of the length of a given MCS will be NANs or similar, and so the compression ratio of files on disk is very high. Hence, even though a file for a given year is only approximately 500MB on disk, its size in memory will be far larger. Fully loading one year using `xr.load_dataset` uses approximately 16G of memory. It is therefore sensible to use `xr.open_dataset` or `xr.open_mfdataset`, which access the dataset's metadata but do not load its data until they are needed.\n",
    "\n",
    "Note, the compression level of a field can be seen from `xarray.Dataset`: `dstracks.area.encoding`. The tracks dataset is compressed using compression level 4.\n",
    "\n",
    "### Pixel-level files\n",
    "\n",
    "The pixel-level files are HEALPix format Zarr store files. The OLR (`rlut`) and precipitation (`pr`) data are part of the model catalog, while the MCS mask data (`mcs_mask_hp(x)_`) is remapped from lat/lon grid used in PyFLEXTRKR during tracking. The MCS mask data is being added to the catalog.\n",
    "\n",
    "### Using `xarray` to access data\n",
    "\n",
    "`xarray` is a convenient way of loading NetCDF or Zarr files in Python. Fields can generally be manipulated using `xarray` methods or by loading the values a `numpy` arrays and manipulating those. This notebook requires having the following Python packages correctly installed (using e.g. `conda`).\n",
    "\n",
    "### Installing pyflextrkr ###\n",
    "\n",
    "This notebook uses a pyflextrkr function to smooth MCS trajectories. Since pyflextrkr is not included in the standard hackathon environment, please install it manually, if you want to use that function. On the Jupyter notebook website open a new tab and choose the terminal. In the terminal activate your hackathon environment. After activating the environment install the package with:\n",
    "\n",
    "`conda install pyflextrkr`\n",
    "\n",
    "If you already created a notebook kernel from your hackathon environment, pyflextrkr should be available in your notebooks now. If you have not installed the kernel from your hackathon environment, install the kernel with:\n",
    "\n",
    "`python -m ipykernel install --user --name=name-of-environment`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e8f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "import cftime\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.geodesic\n",
    "import cartopy.feature as cf\n",
    "import dask\n",
    "from IPython.display import clear_output\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import easygems.healpix as egh\n",
    "import intake\n",
    "from pyflextrkr.smooth_trajectory import smooth_trajectory\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 72  # Set figure resolution to reduce file size\n",
    "# Use for .mp4 video:\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "# Use for javascript animation:\n",
    "# plt.rcParams[\"animation.html\"] = \"jshtml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6cb465",
   "metadata": {},
   "source": [
    "### Load the catalog\n",
    "We have one data catalog for the global hackathon, listing our datasets.\n",
    "But as we have multiple hosting sites, which have some datasets available locally and can access other datasets remotely, the best way to access data may be dependent on the location **where analysis code is executed**.\n",
    "To solve this issue, we have one sub-catalog per hackathon node (the site where analysis code is executed), and an additional `online` catalog, which is available from the public internet. Here's how you can see our currently available sub-catalogs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1acd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(intake.open_catalog(\"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b60305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NERSC catalog\n",
    "current_location = \"UK\"\n",
    "cat = intake.open_catalog(\"https://digital-earths-global-hackathon.github.io/catalog/catalog.yaml\")[current_location]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71cea71",
   "metadata": {},
   "source": [
    "### Pick a Data Set\n",
    "Use `.describe()` on a dateset to see the other parameter options (we use `pandas` just for concise output formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965259cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cat[\"um_glm_n2560_RAL3p3\"].describe()[\"user_parameters\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102096d9",
   "metadata": {},
   "source": [
    "### Load HEALPix Data into a DataSet\n",
    "Two simulations have been tracked. You can choose the N2560 with `zoom` level 10 [(~6km)](https://easy.gems.dkrz.de/Processing/healpix/index.html#healpix-spatial-resolution) or the N1280 with zoom level 9. The datasets containhourly OLR & precipitation, which was used in the MCS tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#N2560\n",
    "#runid='um_glm_n2560_RAL3p3'\n",
    "runid='um_glm_n1280_CoMA9_TBv1p2'\n",
    "\n",
    "dshp = cat[runid](zoom=9).to_dask() \n",
    "dshp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be79acbd-9be7-4de9-844b-b0853217a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get timestep of the data in hours\n",
    "dt=dshp.time.values[1].astype('datetime64[h]')-dshp.time.values[0].astype('datetime64[h]')\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736ac56",
   "metadata": {},
   "source": [
    "### Load MCS Tracking Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f230ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCS data\n",
    "\n",
    "if runid=='um_glm_n2560_RAL3p3':\n",
    "  maskdir = \"https://hackathon-o.s3-ext.jc.rl.ac.uk/sim-data/analysis/PyFLEXTRKR/um_glm_n2560_RAL3p3/mcstracking/mcs_mask_hp10_20200201.0000_20210301.0000.zarr\"\n",
    "  statsdir = \"https://hackathon-o.s3-ext.jc.rl.ac.uk/sim-data/analysis/PyFLEXTRKR/um_glm_n2560_RAL3p3/stats/\"\n",
    "elif runid=='um_glm_n1280_CoMA9_TBv1p2':\n",
    "  maskdir = \"https://hackathon-o.s3-ext.jc.rl.ac.uk/sim-data/analysis/PyFLEXTRKR/um_glm_n1280_CoMA9_TBv1p2_catalog_par/mcstracking/mcs_mask_hp9_20200201.0000_20210301.0000.zarr\"\n",
    "  statsdir = \"https://hackathon-o.s3-ext.jc.rl.ac.uk/sim-data/analysis/PyFLEXTRKR/um_glm_n1280_CoMA9_TBv1p2_catalog_par/stats/\"\n",
    "else:\n",
    "  print(f\"Error: Dataset {runid} not available.\")\n",
    "\n",
    "# MCS track statistics file\n",
    "stats_file = f\"{statsdir}mcs_tracks_final_20200201.0000_20210301.0000.nc\"\n",
    "stats_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc57bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(stats_file, stream=True)\n",
    "\n",
    "# chunks_tracks = {'tracks': 1000, 'times': 400}\n",
    "chunks_tracks = None\n",
    "\n",
    "# Open the dataset\n",
    "dstracks = xr.open_dataset(\n",
    "    BytesIO(response.content), \n",
    "    chunks=chunks_tracks,\n",
    "    mask_and_scale=True)\n",
    "\n",
    "# # Each seperate file for each year defines its own index for tracks. Re-index with a global index.\n",
    "# dstracks[\"tracks\"] = np.arange(0, dstracks.dims[\"tracks\"], 1, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0c60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dstracks.chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d196cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if the dataset uses dask\n",
    "# print(f\"Dataset uses dask: {dstracks.chunks is not None}\")\n",
    "\n",
    "# # Check if variables are dask arrays\n",
    "# for var_name in list(dstracks.data_vars)[:]:  # First 3 variables as example\n",
    "#     print(f\"Variable {var_name} is dask array: {isinstance(dstracks[var_name].data, dask.array.Array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The times have a small offset from the exact times -- e.g. 34500 ns off. Correct this.\n",
    "# This mostly applies to the satellite data\n",
    "def round_times_to_nearest_second(dstracks, fields):\n",
    "    def remove_time_incaccuracy(t):\n",
    "        # To make this an array operation, you have to use the ns version of datetime64, like so:\n",
    "        return (np.round(t.astype(int) / 1e9) * 1e9).astype(\"datetime64[ns]\")\n",
    "\n",
    "    for field in fields:\n",
    "        dstracks[field].load()\n",
    "        tmask = ~np.isnan(dstracks[field].values)\n",
    "        dstracks[field].values[tmask] = remove_time_incaccuracy(\n",
    "            dstracks[field].values[tmask]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "round_times_to_nearest_second(dstracks, ['base_time', 'start_basetime', 'end_basetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b9adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dstracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b848f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, 'NaT' means nan (track does not exist at those times)\n",
    "dstracks.base_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values can be accessed as a `numpy` array:\n",
    "dstracks.ccs_area.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression level info:\n",
    "dstracks.ccs_area.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c78384",
   "metadata": {},
   "source": [
    "### Selecting tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9bd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single track can be selected from its track number:\n",
    "track = dstracks.sel(tracks=20)\n",
    "track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcdd937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might want to select tracks based on e.g. the time at which they are active:\n",
    "datetime = pd.Timestamp('2020-02-01 12:00').to_numpy()\n",
    "# `isel` selects on index. The expression on the RHS collapses the 2D field of `base_time` into a \n",
    "# 1D boolean field that is true if *any* base_time for a given track matches `datetime`.\n",
    "dstracks_at_time = dstracks.isel(\n",
    "     tracks=(dstracks.base_time.values == datetime).any(axis=1)\n",
    ")\n",
    "dstracks_at_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or access tracks based on their location:\n",
    "# N.B. force a load of meanlat.\n",
    "dstracks.meanlat.load()\n",
    "# This suppresses a warning about chunk sizes.\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "    dstracks_tropical = dstracks.isel(\n",
    "        tracks=((dstracks.meanlat.values > -20) & (dstracks.meanlat.values < 20)).any(axis=1)\n",
    "    )\n",
    "dstracks_tropical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68678b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each track can then be looped over using e.g.:\n",
    "for track_id in dstracks_at_time.tracks.values[:10]:  # Just get first 10.\n",
    "    track = dstracks_at_time.sel(tracks=track_id)\n",
    "    print(f\"Track ID: {track.tracks.values.item()}, duration: {track.track_duration.values.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b339a5",
   "metadata": {},
   "source": [
    "### Individual track properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce8fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a track.\n",
    "track = dstracks.sel(tracks=50)\n",
    "track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe83ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access some of the track's properties:\n",
    "# Scalars can be accessed using:\n",
    "duration = track.track_duration.values.item()  # `.values.item()` unpacks the value into an int (in this case).\n",
    "print(f'track_duration: {duration*dt}')\n",
    "# For times, it is useful to convert from a np.datetime64[ns] to a pandas Timestamp object or native datetime object:\n",
    "start_basetime = pd.Timestamp(track.start_basetime.values.item()).to_pydatetime()\n",
    "print(f'start_basetime: {start_basetime}')\n",
    "end_basetime = pd.Timestamp(track.end_basetime.values.item()).to_pydatetime()\n",
    "print(f'end_basetime: {end_basetime}')\n",
    "\n",
    "# Lifetime values can be accessed:\n",
    "# Note, e.g. area values are nan after the duration of the track:\n",
    "print(f'area (full): {track.area.values}')\n",
    "# Slice based on duration:\n",
    "print(f'area (sliced): {track.area.values[:duration]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3499581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiple variables for this track\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "track.ccs_area.plot.line(ax=ax1, label='CCS Area')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "track.corecold_mintb.plot(ax=ax2, color='green', label='Min Tb')\n",
    "ax2.spines['right'].set_position(('outward', 60))  # Move the third y-axis to the right\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "ax3 = ax1.twinx()\n",
    "track.pf_maxrainrate.isel(nmaxpf=0).plot(ax=ax3, color='orange', label='Max Rain Rate')\n",
    "ax3.set_title('')\n",
    "\n",
    "ax4 = ax1.twinx()\n",
    "track.total_rain.plot(ax=ax4, color='red', label='Total Rain')\n",
    "ax4.spines['right'].set_position(('outward', 120))  # Move the third y-axis to the right\n",
    "\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergers data are 2D fields (with -9999 indicating no values for merger number N):\n",
    "track.merge_cloudnumber.values[:duration, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf803ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly for PF data (with nan indicating no value):\n",
    "track.pf_rainrate.values[:duration, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8ba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple plot of the track's position can be made using:\n",
    "plt.scatter(track.meanlon.values[0], track.meanlat.values[0], marker='o', color='k')  # Start point.\n",
    "plt.scatter(track.meanlon.values[duration - 1], track.meanlat.values[duration - 1], marker='x', color='darkorange')  # End point.\n",
    "# Plot the track's path:\n",
    "plt.plot(track.meanlon.values, track.meanlat.values, label='Original Track Path')\n",
    "# Smooth the trajectory using the `smooth_trajectory` function:\n",
    "time_resolution = dstracks.attrs['time_resolution_hour']\n",
    "max_speed_kmh = 100\n",
    "lon_s, lat_s = smooth_trajectory(track.meanlon.values, track.meanlat.values, max_speed_kmh=max_speed_kmh, time_step_h=time_resolution)\n",
    "plt.plot(lon_s, lat_s, color='red', label='Smoothed Track Path')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ccs_area_swath(ax, track, n_points=20):\n",
    "    \"\"\"Adds an area swath of the cold cloud system area, treating each CCS as a circle.\"\"\"\n",
    "    try:\n",
    "        # N.B. these are optional dependencies.\n",
    "        import shapely.geometry\n",
    "        import shapely.ops\n",
    "    except ImportError:\n",
    "        print('shapely not installed')\n",
    "        return\n",
    "    duration = track.track_duration.values.item()\n",
    "    time_indices = range(duration)\n",
    "    \n",
    "    # geoms will contain all the circles.\n",
    "    geoms = []\n",
    "    for i in time_indices:\n",
    "        lon = track.meanlon.values[i]\n",
    "        lat = track.meanlat.values[i]\n",
    "        radius = np.sqrt(track.ccs_area.values[i] / np.pi) * 1e3\n",
    "        circle_points = cartopy.geodesic.Geodesic().circle(\n",
    "            lon=lon, lat=lat, radius=radius, n_samples=n_points, endpoint=False\n",
    "        )\n",
    "        geom = shapely.geometry.Polygon(circle_points)\n",
    "        geoms.append(geom)\n",
    "    # Combine all the circles into a CCS swath.\n",
    "    full_geom = shapely.ops.unary_union(geoms)\n",
    "    ax.add_geometries(\n",
    "        (full_geom,),\n",
    "        crs=cartopy.crs.PlateCarree(),\n",
    "        facecolor=\"none\",\n",
    "        edgecolor=\"royalblue\",\n",
    "        linewidth=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e1d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "extent = track.meanlon.min().item() - 10, track.meanlon.max().item() + 10, track.meanlat.min().item() - 10, track.meanlat.max().item() + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicer figure using cartopy projections and showing a circle based on the CCS area.\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "fig.set_size_inches((8, 4))\n",
    "ax.coastlines()\n",
    "ax.add_feature(cf.BORDERS, linewidth=0.4)\n",
    "ax.scatter(track.meanlon.values[0], track.meanlat.values[0], marker='o', color='k')\n",
    "ax.scatter(track.meanlon.values[duration - 1], track.meanlat.values[duration - 1], marker='x', color='darkorange')\n",
    "ax.plot(track.meanlon.values, track.meanlat.values)\n",
    "ax.plot(lon_s, lat_s, color='red')\n",
    "add_ccs_area_swath(ax, track)\n",
    "buffer = 10\n",
    "extent = track.meanlon.min().item() - buffer, track.meanlon.max().item() + buffer, track.meanlat.min().item() - buffer, track.meanlat.max().item() + buffer\n",
    "ax.set_extent(extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b077c2b",
   "metadata": {},
   "source": [
    "### Group properties of tracks\n",
    "\n",
    "Group properties for lots of tracks can be easily calculated by accessing the fields on an `xarray.Dataset` that contains many tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d81cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean track duration:\n",
    "dstracks.track_duration.values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing using xarray:\n",
    "dstracks.track_duration.mean().values.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f71f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tropical duration:\n",
    "dstracks_tropical.track_duration.values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean area for each track:\n",
    "# `np.nanX` functions are useful for naturally dealing with the missing data.\n",
    "mean_areas = np.nanmean(dstracks.ccs_area.values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24065fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mean_areas, bins=np.linspace(0, 1e6, 101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3000a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing using xarray:\n",
    "xr_mean_areas = dstracks.ccs_area.mean(dim='times', skipna=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a49d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_areas == xr_mean_areas).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97643ff4",
   "metadata": {},
   "source": [
    "## Accessing pixel-level data\n",
    "\n",
    "Pixel-level data is stored separately from the MCS tracks data. These files contain the mcs_mask field, which is area covered by the MCS and can be linked to a given track (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219cd77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MCS mask data:\n",
    "#dsmask = xr.open_dataset(\n",
    "dsmask = xr.open_zarr(\n",
    "    maskdir,\n",
    "    chunks={},\n",
    "    # mask_and_scale=False,\n",
    ")\n",
    "dsmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef55932-2aa3-41e2-896e-0780c0b2892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp(dshp.time.values[0]).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d8f99-0e27-448a-95af-815a6ead0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp(dsmask.time.values[0]).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time components from datasets to deal with different calendars\n",
    "hp_times = [(pd.Timestamp(t).year, pd.Timestamp(t).month, \n",
    "               pd.Timestamp(t).day, pd.Timestamp(t).hour) for t in dshp.time.values]\n",
    "mask_times = [(pd.Timestamp(t).year, pd.Timestamp(t).month, \n",
    "               pd.Timestamp(t).day, pd.Timestamp(t).hour) \n",
    "              for t in dsmask.time.values]\n",
    "\n",
    "# Find matching indices\n",
    "matching_indices = [i for i, t in enumerate(hp_times) if t in set(mask_times)]\n",
    "# Select using indices\n",
    "dshp_subset = dshp.isel(time=matching_indices)\n",
    "# Verify number of timestamps\n",
    "print(f\"Original HEALPix time count: {dshp.time.size}\")\n",
    "print(f\"Matching times found: {len(matching_indices)}\")\n",
    "print(f\"dsmask time count: {dsmask.time.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57569711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dsmask with the time coordinate from dshp_subset\n",
    "dsmask_ = dsmask.copy()\n",
    "\n",
    "# Replace the time coordinate with the matching times from dshp_subset\n",
    "# This must maintain the same order as in the original dsmask\n",
    "dsmask_ = dsmask_.assign_coords(time=dshp_subset.time.values)\n",
    "\n",
    "# Combine the datasets\n",
    "ds = xr.merge([dshp_subset, dsmask_], combine_attrs='drop_conflicts')\n",
    "\n",
    "# Rechunk the dataset to match the mask\n",
    "ds = ds.chunk(dict(dsmask_.chunks))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fa3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot of global OLR\n",
    "im = egh.healpix_show(ds.isel(time=0).rlut, vmin=80, vmax=300, cmap='Greys')\n",
    "plt.colorbar(im, label=f\"{ds.rlut.attrs['long_name']} ({ds.rlut.attrs['units']})\", orientation='horizontal')\n",
    "time_str = ds.time.isel(time=0).dt.strftime(\"%Y-%m-%d %H:%M\").values.item()\n",
    "plt.title(time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db099d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot of global precipitation\n",
    "levels_pr = [0.5,1,2,4,8,16,32,64]\n",
    "cmap = mpl.colormaps.get_cmap('jet')\n",
    "norm = mpl.colors.BoundaryNorm(boundaries=levels_pr, ncolors=cmap.N)\n",
    "# Precipitation\n",
    "pcp_convert=3600 \n",
    "_pr = ds.isel(time=0).pr * pcp_convert # Convert unit from kg/m^2/s to mm/hr, if you use a different model check the units of your precip output \n",
    "_pr = _pr.where(_pr >= np.min(levels_pr), np.nan)\n",
    "\n",
    "im = egh.healpix_show(_pr, norm=norm, cmap=cmap)\n",
    "plt.colorbar(im, label=f\"{ds.pr.attrs['long_name']} (mm/h)\", orientation='horizontal')\n",
    "time_str = ds.time.isel(time=0).dt.strftime(\"%Y-%m-%d %H:%M\").values.item()\n",
    "plt.title(time_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a4073",
   "metadata": {},
   "source": [
    "## Linking tracks to pixel-level data\n",
    "\n",
    "Every track has a corresponding track number in the pixel-level mask data. This can be used to link each track to its pixel-level data, using the timestamp and the track number. The timestamp can be used to determine which time step data to load, and the track number references the equivalent field in the HEALPix dataset (as shown in the figure above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b662e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "itrack = 6\n",
    "\n",
    "\n",
    "# Select a tropical track from Feb 2020, that lasted more than buffer hours and less than 40.\n",
    "dates = pd.DatetimeIndex(dstracks_tropical.start_basetime.values)  # These containers make it easy to select on year, month...\n",
    "track = dstracks_tropical.isel(tracks=(\n",
    "    (dates.year == 2020) & \n",
    "    (dates.month == 2) & \n",
    "    (dstracks_tropical.track_duration > 20) & \n",
    "    (dstracks_tropical.track_duration < 40)\n",
    ")).isel(tracks=itrack)  # just select one track that meets criteria.\n",
    "duration = track.track_duration.values.item()\n",
    "duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f480a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with 1 row and 2 columns\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Create left subplot (regular axis)\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "track.ccs_area.plot(ax=ax1, label='CCS Area')\n",
    "ax1b = ax1.twinx()\n",
    "track.pf_area.isel(nmaxpf=0).plot(ax=ax1b, color='green', label='PF Area')\n",
    "# ax1b.spines['right'].set_position(('outward', 60))  # Move the third y-axis to the right\n",
    "# ax1b.invert_yaxis()\n",
    "ax1b.set_title('')\n",
    "ax1.legend()\n",
    "\n",
    "# Create right subplot (with Cartopy projection)\n",
    "# Use PlateCarree projection (equirectangular projection)\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
    "\n",
    "ax2.coastlines()\n",
    "ax2.add_feature(cf.BORDERS, linewidth=0.4)\n",
    "\n",
    "ax2.scatter(track.meanlon.values[0], track.meanlat.values[0], marker='o', color='k')\n",
    "ax2.scatter(track.meanlon.values[duration - 1], track.meanlat.values[duration - 1], marker='x', color='darkorange')\n",
    "ax2.plot(track.meanlon.values, track.meanlat.values)\n",
    "add_ccs_area_swath(ax2, track)\n",
    "buffer = 8\n",
    "extent = track.meanlon.min().item()-buffer, track.meanlon.max().item()+buffer, track.meanlat.min().item() - buffer, track.meanlat.max().item() + buffer\n",
    "ax2.set_extent(extent)\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.tight_layout(rect=[0, 0.03, 1, 0.95])  # Make room for suptitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b037cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cftime(datetime, calendar):\n",
    "    \"\"\"\n",
    "    Convert a pandas.Timestamp object to a cftime object based on the calendar type.\n",
    "\n",
    "    Args:\n",
    "        datetime: pandas.Timestamp\n",
    "            Timestamp object to convert.\n",
    "        calendar: str\n",
    "            Calendar type.\n",
    "\n",
    "    Returns:\n",
    "        cftime object.\n",
    "    \"\"\"\n",
    "    if calendar == 'noleap':\n",
    "        return cftime.DatetimeNoLeap(datetime.year, datetime.month, datetime.day, datetime.hour, datetime.minute)\n",
    "    elif calendar == 'gregorian':\n",
    "        return cftime.DatetimeGregorian(datetime.year, datetime.month, datetime.day, datetime.hour, datetime.minute)\n",
    "    elif calendar == 'proleptic_gregorian':\n",
    "        return cftime.DatetimeProlepticGregorian(datetime.year, datetime.month, datetime.day, datetime.hour, datetime.minute)\n",
    "    elif calendar == 'standard':\n",
    "        return cftime.DatetimeGregorian(datetime.year, datetime.month, datetime.day, datetime.hour, datetime.minute)\n",
    "    elif calendar == '360_day':\n",
    "        return cftime.Datetime360Day(datetime.year, datetime.month, datetime.day, datetime.hour, datetime.minute)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported calendar type: {calendar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These can be used to work out which pixel-level data to locate the times.\n",
    "calendar = ds['time'].dt.calendar\n",
    "\n",
    "base_times = track.base_time.values[:duration]\n",
    "track_dates = [convert_to_cftime(pd.Timestamp(d).to_pydatetime(), calendar) for d in base_times]\n",
    "print(len(track_dates))\n",
    "track_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aecfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track number in the mask file is +1 offset to the track index in the stats file.\n",
    "track_number = track.tracks + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the animation embed limit [MB]\n",
    "mpl.rcParams['animation.embed_limit'] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18440ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Prev line ensures figure not shown until animation.fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "# Set up a figure to use for the animation below.\n",
    "\n",
    "margin_degree = 10\n",
    "minlon = track.meanlon.values[:duration].min()\n",
    "maxlon = track.meanlon.values[:duration].max()\n",
    "minlat = track.meanlat.values[:duration].min()\n",
    "maxlat = track.meanlat.values[:duration].max()\n",
    "aspect = (maxlon - minlon + 2 * margin_degree) / (maxlat - minlat + 2 * margin_degree)\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "fig.set_size_inches((16, 16 / aspect))\n",
    "fig.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f068b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an animation of the track, MCS, OLR, and precip.\n",
    "def plot_track_link_pixel(i):\n",
    "    try:\n",
    "        print(f'{i + 1}/{duration}')\n",
    "        # print(f'{i}/{duration}')\n",
    "        ax.clear()\n",
    "\n",
    "        ax.set_extent((minlon - margin_degree, maxlon + margin_degree, minlat - margin_degree, maxlat + margin_degree))\n",
    "        ax.coastlines(zorder=3)\n",
    "        ax.add_feature(cf.BORDERS, linewidth=0.4, zorder=3)\n",
    "\n",
    "        date = track_dates[i]\n",
    "        ax.set_title(date)\n",
    "        # Subset the HEALPix dataset to the current date & time\n",
    "        _ds = ds.sel(time=track_dates[i], method='nearest')\n",
    "\n",
    "        # Colormaps\n",
    "        cmap_pr = mpl.colormaps.get_cmap('jet')\n",
    "        cmap_rlut = mpl.colormaps.get_cmap('Greys')\n",
    "        levels_pr = [0.5,1,2,4,8,16,32,64]\n",
    "        norm = mpl.colors.BoundaryNorm(boundaries=levels_pr, ncolors=cmap_pr.N)\n",
    "        # Precipitation\n",
    "        _pr = _ds.pr * pcp_convert  # Convert unit to mm/hr\n",
    "        _pr = _pr.where(_pr >= np.min(levels_pr), np.nan)\n",
    "        # Mask\n",
    "        _mask = _ds.mcs_mask.load()\n",
    "        _mask = _mask.where(_mask == track_number, 1, 0)\n",
    "        Zm_mask = np.ma.masked_where(_mask == 0, _mask)\n",
    "        # Colorfill OLR and precipitation\n",
    "        im_rlut = egh.healpix_show(_ds.rlut, ax=ax, vmin=80, vmax=260, cmap=cmap_rlut, alpha=0.7, zorder=1)\n",
    "        im_pr = egh.healpix_show(_pr, ax=ax, cmap=cmap_pr, norm=norm, alpha=0.9, zorder=1)\n",
    "        im_mask = egh.healpix_show(Zm_mask, ax=ax, cmap='Reds', vmin=1, vmax=1.01, alpha=0.5, zorder=2)\n",
    "        # Contour MCS mask (boundary)\n",
    "        # im_mask = egh.healpix_contour(_mask, ax=ax, levels=[0.5], colors=['r'], linewidths=1, alpha=1)\n",
    "        buffer=8\n",
    "        extent = track.meanlon.min().item()-buffer, track.meanlon.max().item()+buffer, track.meanlat.min().item() - buffer, track.meanlat.max().item() + buffer\n",
    "        ax.set_extent(extent)\n",
    "        \n",
    "        # Display track path.\n",
    "        ax.scatter(track.meanlon.values[0], track.meanlat.values[0], marker='^', c='maroon', zorder=3)\n",
    "        ax.scatter(track.meanlon.values[duration - 1], track.meanlat.values[duration - 1], marker='x', c='maroon', zorder=3)\n",
    "        ax.scatter(track.meanlon.values[i], track.meanlat.values[i], marker='o', c='firebrick', zorder=3)\n",
    "        ax.plot(track.meanlon.values, track.meanlat.values, 'r-', zorder=2)\n",
    "        # clear_output(wait=True)\n",
    "\n",
    "        return ax\n",
    "    except Exception as e:\n",
    "        print(f\"Error in frame {i}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b586f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animation with specific settings\n",
    "anim = matplotlib.animation.FuncAnimation(\n",
    "    fig, \n",
    "    plot_track_link_pixel, \n",
    "    frames=duration,  # Number of frames\n",
    "    # frames=2,\n",
    "    interval=500,\n",
    "    blit=False,  # Set to False for complex plots with multiple elements\n",
    "    cache_frame_data=False  # Disable caching to avoid memory issues\n",
    ")\n",
    "\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab451be",
   "metadata": {},
   "source": [
    "* Track: red line with circle showing its centroid.\n",
    "* MCS mask: red shading.\n",
    "* OLR: grey filled.\n",
    "* precipitation: color filled contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce48b5d9-12f5-4091-8b71-059a2bdbef97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
