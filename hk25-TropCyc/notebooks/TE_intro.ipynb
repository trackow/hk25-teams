{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea896db9-6547-4be5-8862-cfd13443d297",
   "metadata": {},
   "source": [
    "# Track cyclones with TempestExtremes within JASMIN Jupyterhub\n",
    "TempestExtremes is a C++ software for detecting and manipulating features in climate data.  \n",
    "The Software is described two papers:\n",
    "* Ullrich, P.A., C.M. Zarzycki, E.E. McClenny, M.C. Pinheiro, A.M. Stansfield and K.A. Reed (2021) \"TempestExtremes v2.1: A community framework for feature detection, tracking and analysis in large datasets\" Geosci. Model. Dev. 14, pp. 5023â€“5048, doi: 10.5194/gmd-14-5023-2021.\n",
    "* Ullrich, P.A. and C.M. Zarzycki (2017) \"TempestExtremes v1.0: A framework for scale-insensitive pointwise feature tracking on unstructured grids\" Geosci. Model. Dev. 10, pp. 1069-1090, doi: 10.5194/gmd-10-1069-2017.\n",
    "\n",
    "And you can find the documentation here: Please find documentation here: https://climate.ucdavis.edu/tempestextremes.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078085fc-ae8c-4d56-a808-5dc164def4ca",
   "metadata": {},
   "source": [
    "### To do in terminal before the notebook\n",
    "A. If you don't already have a dedicated environment.\n",
    "1. Create a conda environment\n",
    "```\n",
    "conda create -n tempestextremes\n",
    "conda init\n",
    "bash\n",
    "conda activate tempestextremes\n",
    "```\n",
    "2. Install kernels to work with the notebooks\n",
    "```\n",
    "conda install ipykernels\n",
    "python -m ipykernel install --user --name=tempestextremes\n",
    "conda install bash_kernel\n",
    "python -m bash_kernel.install\n",
    "```\n",
    "3. Restart jupyterhub\n",
    "4. (Still in terminal) Install TempestExtremes in the environment\n",
    "```\n",
    "conda init\n",
    "bash\n",
    "conda activate tempestextremes\n",
    "conda install -c conda-forge tempest-extremes\n",
    "```\n",
    "\n",
    "B. If you already created a hackathon-specific conda environment (following e.g. https://digital-earths-global-hackathon-uk.github.io/#software-stack)\n",
    "1. `conda activate hackathon`\n",
    "2. `python -m ipykernel install --user --name=name-of-environment` (If not already done)\n",
    "3. `conda install bash_kernel`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3bb599-ff5c-4a80-a42b-c42f16c6e864",
   "metadata": {},
   "source": [
    "### This notebook\n",
    "NB: Open this notebook with bash kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bef9dc9-9f53-4e27-aae0-79562595fc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate the conda environment in which tempestextremes has been install\n",
    "conda activate tempestextremes\n",
    "# If you have a message saying you need to initalize conda: \n",
    "## create a new cell with the `conda init` command\n",
    "## Run that cell\n",
    "## Delete the cell\n",
    "## Restart your kernel\n",
    "## Run the notebook again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703b4342-c067-4367-9586-dd386ea286bc",
   "metadata": {},
   "source": [
    "To track cyclone with TempestExtremes, you need to first run `DetectNodes` to find suitable \"candidate nodes\" (points in space and time that could be cyclones), and second run `StitchNodes` to \"Stitch\" the candidate nodes into tracks. In this notebook, we show some minimal examples of how that works, and then provide the code for the UZ (Ullrich & Zarzycki, sometimes also known as the eponymous \"TempestExtremes\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c46a21-43a7-4893-96a8-35bbc2b00bf2",
   "metadata": {},
   "source": [
    "## Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4673fd-b8e3-4a4a-a463-b926e804d78d",
   "metadata": {},
   "source": [
    "### Minimal `DetectNodes` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbb22cc-52dd-4847-83ea-607416d62041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments:\n",
      "  --in_data <string> [\"/badc/ecmwf-era5/data/oper/an_sfc/1996/11/18/ecmwf-era5_oper_an_sfc_199611180000.msl.nc\"] \n",
      "  --in_data_list <string> [\"\"] \n",
      "  --in_connect <string> [\"\"] \n",
      "  --diag_connect <bool> [false] \n",
      "  --out <string> [\"nodes.txt\"] \n",
      "  --out_file_list <string> [\"\"] \n",
      "  --searchbymin <string> [\"msl\"] (default PSL)\n",
      "  --searchbymax <string> [\"\"] \n",
      "  --searchbythreshold <string> [\"\"] \n",
      "  --minlon <double> [0.000000] (degrees)\n",
      "  --maxlon <double> [0.000000] (degrees)\n",
      "  --minlat <double> [0.000000] (degrees)\n",
      "  --maxlat <double> [0.000000] (degrees)\n",
      "  --minabslat <double> [0.000000] (degrees)\n",
      "  --mergedist <double> [0.000000] (degrees)\n",
      "  --closedcontourcmd <string> [\"\"] [var,delta,dist,minmaxdist;...]\n",
      "  --noclosedcontourcmd <string> [\"\"] [var,delta,dist,minmaxdist;...]\n",
      "  --thresholdcmd <string> [\"\"] [var,op,value,dist;...]\n",
      "  --outputcmd <string> [\"\"] [var,op,dist;...]\n",
      "  --timestride <integer> [1] \n",
      "  --timefilter <string> [\"\"] \n",
      "  --latname <string> [\"latitude\"] \n",
      "  --lonname <string> [\"longitude\"] \n",
      "  --regional <bool> [false] \n",
      "  --out_header <bool> [false] \n",
      "  --out_seconds <bool> [false] \n",
      "  --logdir <string> [\".\"] \n",
      "  --verbosity <integer> [0] \n",
      "------------------------------------------------------------\n",
      "Begin search operation\n",
      "..Generating RLL grid data\n",
      "....Total calculated grid area: 1.256637061435853e+01 sr\n",
      "....Done\n",
      "..Time 1996-11-18 00:00:00\n",
      "....Total candidates: 8299\n",
      "....Rejected (  location): 0\n",
      "....Rejected (    merged): 0\n",
      "....Rejected ( threshold): 0\n",
      "....Done\n",
      "..Done\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Here we only find SLP minima in one file\n",
    "# The file contains ERA5 SLP for one time step\n",
    "DetectNodes \\\n",
    "--in_data \"/badc/ecmwf-era5/data/oper/an_sfc/1996/11/18/ecmwf-era5_oper_an_sfc_199611180000.msl.nc\" \\\n",
    "--out nodes.txt \\\n",
    "--searchbymin \"msl\" \\\n",
    "--latname \"latitude\" --lonname \"longitude\" \n",
    "# The output will summarize command arguments, and give you information about the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d4382b-7ec6-48b1-af0e-394db1a455f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996\t11\t18\t8299\t0\n",
      "\t600\t0\t150.000000\t90.000000\n",
      "\t601\t0\t150.250000\t90.000000\n",
      "\t602\t0\t150.500000\t90.000000\n",
      "\t603\t0\t150.750000\t90.000000\n",
      "\t604\t0\t151.000000\t90.000000\n",
      "\t605\t0\t151.250000\t90.000000\n",
      "\t606\t0\t151.500000\t90.000000\n",
      "\t607\t0\t151.750000\t90.000000\n",
      "\t608\t0\t152.000000\t90.000000\n"
     ]
    }
   ],
   "source": [
    "# Visualize nodes.txt output file\n",
    "head nodes.txt\n",
    "# For each time step, list of candidate nodes \n",
    "# For each candidate node, contains lon. index, lat. index, lon. value and lat. value\n",
    "# Will contain more columns if more info is requested through the `outputcmd` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "056b1a95-7718-4164-94d4-2ddf76b1d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean \n",
    "rm nodes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a2889a-86ed-4161-91f9-c495286e2e9c",
   "metadata": {},
   "source": [
    "### `DetectNodes` over several files\n",
    "#### Several file of the same variable at different times\n",
    "If you want to search through several files at once, you need to create a text file listing the file paths, and then provide it to `--in_data_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08084a57-2837-4635-8187-a48c892e4e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /badc/ecmwf-era5/data/oper/an_sfc/1996/11/18/*msl* > flist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "133f0977-ac0a-4cc3-9420-04496dade2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store nodes in\n",
    "if ! [ -e nodes ]\n",
    "then\n",
    "    mkdir nodes\n",
    "else \n",
    "    echo \"nodes folder already exist, watch out for potential collisions.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6da75e2-c10e-4d97-8ab5-6c5d395a6aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments:\n",
      "  --in_data <string> [\"\"] \n",
      "  --in_data_list <string> [\"flist.txt\"] \n",
      "  --in_connect <string> [\"\"] \n",
      "  --diag_connect <bool> [false] \n",
      "  --out <string> [\"nodes/\"] \n",
      "  --out_file_list <string> [\"\"] \n",
      "  --searchbymin <string> [\"msl\"] (default PSL)\n",
      "  --searchbymax <string> [\"\"] \n",
      "  --searchbythreshold <string> [\"\"] \n",
      "  --minlon <double> [0.000000] (degrees)\n",
      "  --maxlon <double> [0.000000] (degrees)\n",
      "  --minlat <double> [0.000000] (degrees)\n",
      "  --maxlat <double> [0.000000] (degrees)\n",
      "  --minabslat <double> [0.000000] (degrees)\n",
      "  --mergedist <double> [0.000000] (degrees)\n",
      "  --closedcontourcmd <string> [\"\"] [var,delta,dist,minmaxdist;...]\n",
      "  --noclosedcontourcmd <string> [\"\"] [var,delta,dist,minmaxdist;...]\n",
      "  --thresholdcmd <string> [\"\"] [var,op,value,dist;...]\n",
      "  --outputcmd <string> [\"\"] [var,op,dist;...]\n",
      "  --timestride <integer> [1] \n",
      "  --timefilter <string> [\"\"] \n",
      "  --latname <string> [\"latitude\"] \n",
      "  --lonname <string> [\"longitude\"] \n",
      "  --regional <bool> [false] \n",
      "  --out_header <bool> [false] \n",
      "  --out_seconds <bool> [false] \n",
      "  --logdir <string> [\".\"] \n",
      "  --verbosity <integer> [0] \n",
      "------------------------------------------------------------\n",
      "Begin search operation\n",
      "..Output will be written to nodes/XXXXXX.dat\n",
      "..Logs will be written to ./logXXXXXX.txt\n",
      "..Done\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "DetectNodes \\\n",
    "--in_data_list flist.txt \\\n",
    "--out nodes/ \\\n",
    "--searchbymin \"msl\" \\\n",
    "--latname \"latitude\" --lonname \"longitude\" \n",
    "rm log* # Comment this if you need the log files for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb653d2-4755-46ed-81c2-fda2b191ab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000.dat  000004.dat\t000008.dat  000012.dat\t000016.dat  000020.dat\n",
      "000001.dat  000005.dat\t000009.dat  000013.dat\t000017.dat  000021.dat\n",
      "000002.dat  000006.dat\t000010.dat  000014.dat\t000018.dat  000022.dat\n",
      "000003.dat  000007.dat\t000011.dat  000015.dat\t000019.dat  000023.dat\n"
     ]
    }
   ],
   "source": [
    "ls nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f77163-9eeb-44d2-85a1-5b25f863bba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996\t11\t18\t8299\t0\n",
      "\t600\t0\t150.000000\t90.000000\n",
      "\t601\t0\t150.250000\t90.000000\n",
      "\t602\t0\t150.500000\t90.000000\n",
      "\t603\t0\t150.750000\t90.000000\n",
      "\t604\t0\t151.000000\t90.000000\n",
      "\t605\t0\t151.250000\t90.000000\n",
      "\t606\t0\t151.500000\t90.000000\n",
      "\t607\t0\t151.750000\t90.000000\n",
      "\t608\t0\t152.000000\t90.000000\n"
     ]
    }
   ],
   "source": [
    "# Each file in nodes contains the list of candidate nodes for one input file\n",
    "head nodes/000000.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7276fa7-ece2-4b75-adde-20d658f9a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "rm -rf nodes flist.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac6851-b67e-4cfa-beca-b727d5ec4c8e",
   "metadata": {},
   "source": [
    "#### Several files of different variables at the same time\n",
    "The input filelist must contain all files path separated by a `;`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "257fd67a-9cb5-4839-bd7e-3801265b238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take msl, 10u and 10v files for a signle time step\n",
    "msl_file=\"/badc/ecmwf-era5/data/oper/an_sfc/1996/11/18/ecmwf-era5_oper_an_sfc_199611180000.msl.nc\"\n",
    "u10_file=\"/badc/ecmwf-era5/data/oper/an_sfc/1996/11/18/ecmwf-era5_oper_an_sfc_199611180000.10u.nc\"\n",
    "v10_file=\"/badc/ecmwf-era5/data/oper/an_sfc/1996/11/18/ecmwf-era5_oper_an_sfc_199611180000.10v.nc\"\n",
    "echo ${msl_file}\\;${u10_file}\\;${v10_file} > flist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4a0b9fa-1d37-4ac9-99bc-0a2d89a157c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments:\n",
      "  --in_data <string> [\"\"] \n",
      "  --in_data_list <string> [\"flist.txt\"] \n",
      "  --in_connect <string> [\"\"] \n",
      "  --diag_connect <bool> [false] \n",
      "  --out <string> [\"nodes.txt\"] \n",
      "  --out_file_list <string> [\"\"] \n",
      "  --searchbymin <string> [\"msl\"] (default PSL)\n",
      "  --searchbymax <string> [\"\"] \n",
      "  --searchbythreshold <string> [\"\"] \n",
      "  --minlon <double> [0.000000] (degrees)\n",
      "  --maxlon <double> [0.000000] (degrees)\n",
      "  --minlat <double> [0.000000] (degrees)\n",
      "  --maxlat <double> [0.000000] (degrees)\n",
      "  --minabslat <double> [0.000000] (degrees)\n",
      "  --mergedist <double> [6.000000] (degrees)\n",
      "  --closedcontourcmd <string> [\"\"] [var,delta,dist,minmaxdist;...]\n",
      "  --noclosedcontourcmd <string> [\"\"] [var,delta,dist,minmaxdist;...]\n",
      "  --thresholdcmd <string> [\"\"] [var,op,value,dist;...]\n",
      "  --outputcmd <string> [\"msl,min,0;_VECMAG(u10,v10),max,2\"] [var,op,dist;...]\n",
      "  --timestride <integer> [1] \n",
      "  --timefilter <string> [\"\"] \n",
      "  --latname <string> [\"latitude\"] \n",
      "  --lonname <string> [\"longitude\"] \n",
      "  --regional <bool> [false] \n",
      "  --out_header <bool> [false] \n",
      "  --out_seconds <bool> [false] \n",
      "  --logdir <string> [\".\"] \n",
      "  --verbosity <integer> [0] \n",
      "------------------------------------------------------------\n",
      "Parsing output operations\n",
      "..Minimum of msl within 0.000000 degrees\n",
      "..Maximum of _VECMAG(u10,v10) within 2.000000 degrees\n",
      "..Done\n",
      "Begin search operation\n",
      "..Generating RLL grid data\n",
      "....Total calculated grid area: 1.256637061435853e+01 sr\n",
      "....Done\n",
      "..Time 1996-11-18 00:00:00\n",
      "....Total candidates: 907\n",
      "....Rejected (  location): 0\n",
      "....Rejected (    merged): 7392\n",
      "....Rejected ( threshold): 0\n",
      "....Done\n",
      "..Done\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Note there is now a new lines \"outputcmd\" which specifies which information to add to output\n",
    "DetectNodes \\\n",
    "--in_data_list flist.txt \\\n",
    "--out nodes.txt \\\n",
    "--searchbymin \"msl\" \\\n",
    "--outputcmd \"msl,min,0;_VECMAG(u10,v10),max,2\" \\\n",
    "--mergedist \"6.0\" \\\n",
    "--latname \"latitude\" --lonname \"longitude\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38f71ec0-5cec-4b87-a274-54162a5eeaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996\t11\t18\t907\t0\n",
      "\t600\t0\t150.000000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t601\t0\t150.250000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t602\t0\t150.500000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t603\t0\t150.750000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t604\t0\t151.000000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t605\t0\t151.250000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t606\t0\t151.500000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t607\t0\t151.750000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t608\t0\t152.000000\t90.000000\t9.890202e+04\t1.794951e+01\n"
     ]
    }
   ],
   "source": [
    "# Nodes file now contains two more coluns, with the SLP minimum and the 10m wind maximum\n",
    "head nodes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d09f8db4-1cba-4452-9884-d05e56be03aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "rm nodes.txt flist.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845fc75-9f95-4cc9-8e6e-f2e039b7fd46",
   "metadata": {},
   "source": [
    "#### Several files of different variables over several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f46af5cc-93a4-4dae-8da1-429362a24518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store nodes in\n",
    "if ! [ -e nodes ]; then mkdir nodes; \n",
    "else echo \"nodes folder already exist, watch out for potential collisions.\"; \n",
    "fi\n",
    "# Create folder to store logs in\n",
    "if ! [ -e logs ]; then mkdir logs; \n",
    "else echo \"logs folder already exist, watch out for potential collisions.\";\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffdaddd0-33d2-4d2c-b254-d8100bd7e19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00\n",
      "01\n",
      "02\n",
      "03\n",
      "04\n"
     ]
    }
   ],
   "source": [
    "# Loop over time (in that case hours in a day)\n",
    "for h in 00 01 02 03 04\n",
    "do \n",
    "    echo $h\n",
    "    # Define the files for the different variables\n",
    "    msl_file=\"/badc/ecmwf-era5/data/oper/an_sfc/1996/11/18/ecmwf-era5_oper_an_sfc_19961118${h}00.msl.nc\"\n",
    "    u10_file=\"/badc/ecmwf-era5/data/oper/an_sfc/1996/11/18/ecmwf-era5_oper_an_sfc_19961118${h}00.10u.nc\"\n",
    "    v10_file=\"/badc/ecmwf-era5/data/oper/an_sfc/1996/11/18/ecmwf-era5_oper_an_sfc_19961118${h}00.10v.nc\"\n",
    "    # Concatenate them into flist.txt\n",
    "    echo ${msl_file}\\;${u10_file}\\;${v10_file} > flist.txt\n",
    "    \n",
    "    # Run DetectNodes for this time step\n",
    "    DetectNodes \\\n",
    "        --in_data_list flist.txt \\\n",
    "        --out nodes/${h}.txt \\\n",
    "        --searchbymin \"msl\" \\\n",
    "        --outputcmd \"msl,min,0;_VECMAG(u10,v10),max,2\" \\\n",
    "        --mergedist \"6.0\" \\\n",
    "        --latname \"latitude\" --lonname \"longitude\" > logs/${h}.txt\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce49d217-8817-4be9-b527-a0f2194dd2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00.txt\t01.txt\t02.txt\t03.txt\t04.txt\n"
     ]
    }
   ],
   "source": [
    "ls nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd1ff897-87e2-4c86-adef-2bf41cc9a22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1996\t11\t18\t907\t0\n",
      "\t600\t0\t150.000000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t601\t0\t150.250000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t602\t0\t150.500000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t603\t0\t150.750000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t604\t0\t151.000000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t605\t0\t151.250000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t606\t0\t151.500000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t607\t0\t151.750000\t90.000000\t9.890202e+04\t1.794951e+01\n",
      "\t608\t0\t152.000000\t90.000000\t9.890202e+04\t1.794951e+01\n"
     ]
    }
   ],
   "source": [
    "# Visualise file\n",
    "head nodes/00.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5dc0f06-d4cb-4865-9f44-511c6447ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean\n",
    "rm -rf logs flist.txt\n",
    "# Do no remove nodes : We will use them next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ca2a2-b991-45fc-976d-eb4bfc2e0d14",
   "metadata": {},
   "source": [
    "### Minimal `StitchNodes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e863450-95a9-44ae-8ad1-b275af0107f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file with list of node files you want to use\n",
    "ls nodes/*.txt > flist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "273d5cb7-1dc2-494d-a7e8-266880e907e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments:\n",
      "  --in <string> [\"\"] \n",
      "  --in_list <string> [\"flist.txt\"] \n",
      "  --in_connect <string> [\"\"] \n",
      "  --out <string> [\"tracks.csv\"] \n",
      "  --in_fmt <string> [\"lon,lat,slp,wind10\"] \n",
      "  --range <double> [5.000000] (degrees)\n",
      "  --mintime <string> [\"3\"] \n",
      "  --time_begin <string> [\"\"] \n",
      "  --time_end <string> [\"\"] \n",
      "  --prioritize <string> [\"\"] \n",
      "  --min_endpoint_dist <double> [0.000000] (degrees)\n",
      "  --min_path_dist <double> [0.000000] (degrees)\n",
      "  --maxgap <string> [\"0\"] \n",
      "  --threshold <string> [\"\"] [col,op,value,count;...]\n",
      "  --caltype <string> [\"standard\"] (none|standard|noleap|360_day)\n",
      "  --allow_repeated_times <bool> [false] \n",
      "  --add_velocity <bool> [false] \n",
      "  --out_file_format <string> [\"csv\"] (gfdl|csv|csvnohead)\n",
      "  --out_seconds <bool> [false] \n",
      "------------------------------------------------------------\n",
      "Loading candidate data\n",
      "..File (1/5) \"nodes/00.txt\"\n",
      "..File (2/5) \"nodes/01.txt\"\n",
      "..File (3/5) \"nodes/02.txt\"\n",
      "..File (4/5) \"nodes/03.txt\"\n",
      "..File (5/5) \"nodes/04.txt\"\n",
      "..Discrete times: 5 (1996-11-18 00:00:00 to 1996-11-18 04:00:00)\n",
      "..Done\n",
      "Creating KD trees at each time level.. Done\n",
      "Constructing paths\n",
      "..Populating set of path segments.. Done\n",
      "..Connecting path segments.. Done\n",
      "..Done\n",
      "Filtering paths\n",
      "..Paths rejected (mintime): 85\n",
      "..Paths rejected (minendpointdist): 0\n",
      "..Paths rejected (minpathdist): 0\n",
      "..Paths rejected (threshold): 0\n",
      "..Total paths found: 899\n",
      "..Done\n",
      "Writing results.. Done\n",
      "Cleanup.. Done\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Call minimal StitchNodes\n",
    "StitchNodes \\\n",
    "--in_list flist.txt \\\n",
    "--in_fmt \"lon,lat,slp,wind10\" \\\n",
    "--out \"tracks.csv\" \\\n",
    "--out_file_format \"csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "673992dd-73c0-405a-b7ce-7a365c0cfad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id, year, month, day, hour, i, j, lon, lat, slp, wind10\n",
      "0, 1996, 11, 18, 0, 600, 0, 150.000000, 90.000000, 9.890202e+04, 1.794951e+01\n",
      "0, 1996, 11, 18, 1, 603, 0, 150.750000, 90.000000, 9.909003e+04, 1.812507e+01\n",
      "0, 1996, 11, 18, 2, 604, 0, 151.000000, 90.000000, 9.922524e+04, 1.839613e+01\n",
      "0, 1996, 11, 18, 3, 606, 0, 151.500000, 90.000000, 9.934372e+04, 1.837108e+01\n",
      "0, 1996, 11, 18, 4, 608, 0, 152.000000, 90.000000, 9.944643e+04, 1.804239e+01\n",
      "1, 1996, 11, 18, 0, 604, 0, 151.000000, 90.000000, 9.890202e+04, 1.794951e+01\n",
      "1, 1996, 11, 18, 1, 604, 0, 151.000000, 90.000000, 9.909003e+04, 1.812507e+01\n",
      "1, 1996, 11, 18, 2, 604, 0, 151.000000, 90.000000, 9.922524e+04, 1.839613e+01\n",
      "2, 1996, 11, 18, 0, 605, 0, 151.250000, 90.000000, 9.890202e+04, 1.794951e+01\n"
     ]
    }
   ],
   "source": [
    "# Visualize the tracks file\n",
    "head tracks.csv\n",
    "# For each track it identified, a track_id was defined\n",
    "# the csv contains information about the position in space and time for each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dfdeb3-57e0-4556-af7e-88a2ec0a2de0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
